{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#from wordcloud import WordCloud\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install xgboost\n",
    "    print('Sikerült')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Study of coupling loss on bi-columnar BSCCO/Ag...</td>\n",
       "      <td>0</td>\n",
       "      <td>Coupling losses were studied in composite tape...</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Study of coupling loss on bi-columnar BSCCO/Ag...</td>\n",
       "      <td>1</td>\n",
       "      <td>In this study, we investigate the coupling los...</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Weighted Solyanik estimates for the strong max...</td>\n",
       "      <td>0</td>\n",
       "      <td>Let $\\mathsf M_{\\mathsf S}$ denote the strong ...</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weighted Solyanik estimates for the strong max...</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we investigate Weighted Solyani...</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SOFIA-EXES Observations of Betelgeuse during t...</td>\n",
       "      <td>0</td>\n",
       "      <td>In 2019 October Betelgeuse began a decline in ...</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  label  \\\n",
       "0  Study of coupling loss on bi-columnar BSCCO/Ag...      0   \n",
       "1  Study of coupling loss on bi-columnar BSCCO/Ag...      1   \n",
       "2  Weighted Solyanik estimates for the strong max...      0   \n",
       "3  Weighted Solyanik estimates for the strong max...      1   \n",
       "4  SOFIA-EXES Observations of Betelgeuse during t...      0   \n",
       "\n",
       "                                                text  word_count  \n",
       "0  Coupling losses were studied in composite tape...         280  \n",
       "1  In this study, we investigate the coupling los...         215  \n",
       "2  Let $\\mathsf M_{\\mathsf S}$ denote the strong ...         332  \n",
       "3  In this paper, we investigate Weighted Solyani...         225  \n",
       "4  In 2019 October Betelgeuse began a decline in ...         268  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('research_abstract_labeled.csv', sep=\";\", encoding='cp1252')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    6400\n",
       "1    6400\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Itt kigyűjtöm a címeket, amelyikeknél vagy az AI vagy az ember szöveg tartalmaz dollárjelet\n",
    "formula_titles = data[data.text.str.contains('$', regex=False, na=False)].drop_duplicates(subset=['title'], keep='first').title.tolist()\n",
    "\n",
    "# Kifilterezem mind a két absztraktot ahol legalább az egyikben van dollárjel\n",
    "filtered = data[data.title.isin(formula_titles) != True]\n",
    "\n",
    "filtered.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    12800.000000\n",
       "mean       191.880703\n",
       "std         88.157998\n",
       "min         29.000000\n",
       "25%        120.000000\n",
       "50%        182.000000\n",
       "75%        260.000000\n",
       "max        594.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18148\\2591102394.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered['clean_text'] = filtered['text'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    \"\"\"\n",
    "    Pos tagging\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  \n",
    "\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    \n",
    "    \"\"\"\n",
    "    cleaning, lemmatization and stopword removal\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    text = re.sub(r'\\n+', ' ', text).strip()\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "        for word, tag in pos_tags\n",
    "        if word not in stop_words\n",
    "    ]\n",
    "\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# creating new var with clean texts\n",
    "filtered['clean_text'] = filtered['text'].apply(preprocess_text)\n",
    "\n",
    "#filtered.to_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw (Human):\n",
      "In 2019 October Betelgeuse began a decline in V-band brightness that went beyond the minimum expected from its quasi-periodic ~420 day cycle, becoming the faintest in recorded photometric history. Obs\n",
      "Clean:\n",
      "2019 october betelgeuse begin decline vband brightness go beyond minimum expect quasiperiodic 420 day cycle become faint recorded photometric history observation obtain 2019 december vltsphere montarg\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "SOFIA-EXES Observations of Betelgeuse During the Great Dimming of 2019/2020\n",
      "\n",
      "Betelgeuse, a red supergiant star in the constellation Orion, exhibited a significant dimming event starting in late 2019 t\n",
      "Clean:\n",
      "sofiaexes observation betelgeuse great dimming 20192020 betelgeuse red supergiant star constellation orion exhibit significant dimming event start late 2019 early 2020 phenomenon capture attention ama\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Dust transport and deposition behind larger boulders on the comet 67P/Churyumov-Gerasimenko (67P/C-G) have been observed by the Rosetta mission.\n",
      "\n",
      "We present a mechanism for dust transport vectors base\n",
      "Clean:\n",
      "dust transport deposition behind large boulder comet 67pchuryumovgerasimenko 67pcg observe rosetta mission present mechanism dust transport vector base homogenous surface activity model incorporate de\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This study investigates the prevailing dust-transport directions on comet 67P/Churyumov-Gerasimenko. We analyzed images taken by the Rosetta spacecraft's OSIRIS camera during the comet's closest appro\n",
      "Clean:\n",
      "study investigate prevail dusttransport direction comet 67pchuryumovgerasimenko analyze image take rosetta spacecraft osiris camera comet closest approach sun find dust transport comet surface follow \n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "An innovative millimeter wave diagnostic is proposed to measure the local magnetic field and edge current as a function of the minor radius in the tokamak pedestal region. The idea is to identify the \n",
      "Clean:\n",
      "innovative millimeter wave diagnostic propose measure local magnetic field edge current function minor radius tokamak pedestal region idea identify direction minimum reflectivity omode cutoff layer co\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This paper presents a feasibility study of an anti-radar diagnostic of magnetic fields through a combination of O-X mode conversion and oblique reflectometry imaging. The proposed method relies on the\n",
      "Clean:\n",
      "paper present feasibility study antiradar diagnostic magnetic field combination ox mode conversion oblique reflectometry image propose method relies conversion ordinary mode probe electromagnetic wave\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "High-dispersion spectra centered on the Li 6708 A line have been obtained for 70 potential members of the open cluster NGC 3680, with an emphasis on stars in the turnoff region. A measurable Li abunda\n",
      "Clean:\n",
      "highdispersion spectrum center li 6708 line obtain 70 potential member open cluster ngc 3680 emphasis star turnoff region measurable li abundance derive 53 star 39 radial velocity proper motion consis\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The open cluster, NGC 3680, has been of interest to many astronomers due to its intermediate age and proximity to the Sun. Recently, lithium has also emerged as a topic of great interest within the cl\n",
      "Clean:\n",
      "open cluster ngc 3680 interest many astronomer due intermediate age proximity sun recently lithium also emerge topic great interest within cluster present detailed analysis lithium abundance ngc 3680 \n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Orthogonal Matching Pursuit (OMP) is a simple, yet empirically competitive algorithm for sparse recovery. Recent developments have shown that OMP guarantees exact recovery of K-sparse signals with K o\n",
      "Clean:\n",
      "orthogonal matching pursuit omp simple yet empirically competitive algorithm sparse recovery recent development show omp guarantee exact recovery ksparse signal k k iteration observation matrix satisf\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This paper presents a comprehensive study of online recovery guarantees and analytical results for Orthogonal Matching Pursuit (OMP). We begin by examining the asymptotic analysis of OMP- a well-known\n",
      "Clean:\n",
      "paper present comprehensive study online recovery guarantee analytical result orthogonal matching pursuit omp begin examine asymptotic analysis omp wellknown greedy algorithm use sparse signal recover\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Dynamical generation of 4d gauge theories and gravity at low energy from the 3d ones at high energy is studied, based on the fermion condensation mechanism recently proposed by Arkani-Hamed, Cohen and\n",
      "Clean:\n",
      "dynamical generation 4d gauge theory gravity low energy 3d one high energy study base fermion condensation mechanism recently propose arkanihamed cohen georgi gravity 4d einstein gravity generate mult\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "In this paper, we investigate the relationship between 4d gauge theory and gravity that arises from 3d ones at high energy. We explore the dualities between these theories and their implications for o\n",
      "Clean:\n",
      "paper investigate relationship 4d gauge theory gravity arise 3d one high energy explore duality theory implication understanding fundamental physic use tool holography quantum field theory provide evi\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "The capacity of the semi-deterministic relay channel (SD-RC) with non-causal channel state information (CSI) only at the encoder and decoder is characterized. The capacity is achieved by a scheme base\n",
      "Clean:\n",
      "capacity semideterministic relay channel sdrc noncausal channel state information csi encoder decoder characterize capacity achieve scheme base cooperativebinforward scheme allow cooperation transmitt\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The problem of channel coding in cooperative communication systems is well studied in the literature, but most of the existing work assumes that the channel state information (CSI) is known at the tra\n",
      "Clean:\n",
      "problem channel coding cooperative communication system well study literature exist work assume channel state information csi know transmitter causal fashion however many practical situation csi avail\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Identifying the most influential nodes in information networks has been the focus of many research studies. This problem has crucial applications in various contexts including biology, medicine, and s\n",
      "Clean:\n",
      "identify influential node information network focus many research study problem crucial application various context include biology medicine social science control propagation virus rumour real world \n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The analysis of complex networks is a crucial factor in the understanding and exploration of various systems, from social networks to biological and technological systems. In this study, we present a \n",
      "Clean:\n",
      "analysis complex network crucial factor understanding exploration various system social network biological technological system study present new approach measure influential node rank network use dyn\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "The properties of organometallic wires [TM2(Ant)] constructed with transitional metals (TM = Sc, Ti, V, Cr, Mn and Fe) and anthracene (Ant) are investigated by first-principles calculations. As the ga\n",
      "Clean:\n",
      "property organometallic wire tm2ant construct transitional metal tm sc ti v cr mn fe anthracene ant investigate firstprinciples calculation gap homo high occupied molecular orbital lumo low unoccupied\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The development of organometallic wires has been an important area of research in recent years, due to their potential applications in molecular electronics and nanotechnology. In this study, we aimed\n",
      "Clean:\n",
      "development organometallic wire important area research recent year due potential application molecular electronics nanotechnology study aim investigate electronic optical property transitional metalb\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "The cloud radio access network (C-RAN) concept, in which densely deployed access points (APs) are empowered by cloud computing to cooperatively support mobile users (MUs), to improve mobile data rates\n",
      "Clean:\n",
      "cloud radio access network cran concept densely deploy access point aps empower cloud compute cooperatively support mobile user mus improve mobile data rate recently propose however high density activ\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "Cloud Radio Access Network (C-RAN) is an innovative architecture for wireless communication systems that leverages cloud computing techniques to optimize network performance. A fundamental aspect of C\n",
      "Clean:\n",
      "cloud radio access network cran innovative architecture wireless communication system leverage cloud compute technique optimize network performance fundamental aspect cran design energy efficiency nec\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Results from spectral geometry such as Weyl's formula can be used to relate the thermodynamic properties of a free massless field to the spatial manifold on which it is defined. We begin by calculatin\n",
      "Clean:\n",
      "result spectral geometry weyls formula use relate thermodynamic property free massless field spatial manifold define begin calculate free energy two case manifold posessing boundary spheres subextensi\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This paper investigates the density of states of a free conformal field theory (CFT) and the finite volume corrections associated with such a system. Our approach involves analyzing the spectrum of op\n",
      "Clean:\n",
      "paper investigate density state free conformal field theory cft finite volume correction associate system approach involve analyze spectrum operator correlation function utilize analytical numerical m\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Noncommutative Maxwell-Chern-Simons theory in 3-dimensions is defined in terms of star product and noncommutative fields. Seiberg-Witten map is employed to write it in terms of ordinary fields. A pare\n",
      "Clean:\n",
      "noncommutative maxwellchernsimons theory 3dimensions define term star product noncommutative field seibergwitten map employ write term ordinary field parent action introduce dual action derive spatial\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "In this paper, we investigate the noncommutative Maxwell-Chern-Simons theory in three dimensions. We demonstrate the existence of duality between the electric and magnetic sectors and investigate the \n",
      "Clean:\n",
      "paper investigate noncommutative maxwellchernsimons theory three dimension demonstrate existence duality electric magnetic sector investigate underlying algebraic structure theory introduce new noncom\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "There is an increasing body of evidence that suggests that Be disks are well described by the Viscous Decretion Disk (VDD) model according to which the formation and structure of the disk depend on th\n",
      "Clean:\n",
      "increase body evidence suggest disk well describe viscous decretion disk vdd model accord formation structure disk depend kinematic viscosity gas however observational test vdd todate do system displa\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This research paper presents a multi-technique study of the dynamical evolution of the viscous disk around the Be star ? CMa. The disk around Be stars is a well-known phenomenon in astrophysics and is\n",
      "Clean:\n",
      "research paper present multitechnique study dynamical evolution viscous disk around star cma disk around star wellknown phenomenon astrophysics think cause rapid rotation star result ejection material\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "We present a deep study of the average hard X-ray spectra of Seyfert galaxies. We analyzed all public INTEGRAL IBIS/ISGRI data available on all the 165 Seyfert galaxies detected at z<0.2. Our final sa\n",
      "Clean:\n",
      "present deep study average hard xray spectrum seyfert galaxy analyze public integral ibisisgri data available 165 seyfert galaxy detect z02 final sample consist 44 seyfert 1 29 seyfert 15 78 seyfert 2\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "Seyfert galaxies have been extensively studied in recent years to understand the mechanisms behind active galactic nuclei (AGN). AGN are among the brightest and most energetic sources in the universe,\n",
      "Clean:\n",
      "seyfert galaxy extensively study recent year understand mechanism behind active galactic nucleus agn agn among bright energetic source universe observed property provide valuable information physical \n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "We present an analysis of the Hubble diagram for 12 Type Ia supernovae (SNe Ia) observed in the near-infrared J and H bands. We select SNe exclusively from the redshift range 0.03 < z < 0.09 to reduce\n",
      "Clean:\n",
      "present analysis hubble diagram 12 type ia supernova sne ia observe nearinfrared j h band select sne exclusively redshift range 003 z 009 reduce uncertainty come peculiar velocity remain cosmologicall\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "Type Ia supernovae (SNe Ia) are known to be the most reliable \"standard candles\" used in cosmology. These type of supernovae reach an extremely high temperature and brightness, and their light curves \n",
      "Clean:\n",
      "type ia supernova sne ia know reliable standard candle use cosmology type supernova reach extremely high temperature brightness light curve easily observable visible nearinfrared wavelength despite wa\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "The use of trigonometric polynomials as Lagrange multipliers in the harmonic mortar method enables an efficient and elegant treatment of relative motion in the stator-rotor coupling of electric machin\n",
      "Clean:\n",
      "use trigonometric polynomial lagrange multiplier harmonic mortar method enable efficient elegant treatment relative motion statorrotor coupling electric machine simulation explicit formula torque comp\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This paper presents a novel approach to torque computation in electric machine simulation using harmonic mortar methods. Specifically, we propose a new formulation based on the mortar element method t\n",
      "Clean:\n",
      "paper present novel approach torque computation electric machine simulation use harmonic mortar method specifically propose new formulation base mortar element method yield accurate efficient result e\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Based on work of R. Lazarsfeld and M. Popa, we use the derivative complex associated to the bundle of the holomorphic p-forms to provide inequalities for all the Hodge numbers of a special class of ir\n",
      "Clean:\n",
      "base work r lazarsfeld popa use derivative complex associate bundle holomorphic pforms provide inequality hodge number special class irregular compact kaehler manifold 3folds 4folds give asymptotic bo\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This research article presents new inequalities for the Hodge numbers of irregular compact Kaehler manifolds. The main result of the study details the relationship between the Hodge numbers and the di\n",
      "Clean:\n",
      "research article present new inequality hodge number irregular compact kaehler manifold main result study detail relationship hodge number dimension manifold demonstrate relationship affect overall al\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "--- Objectives --- Prior to foetal karyotyping, the likelihood of Down's syndrome is often determined combining maternal age, serum free beta-HCG, PAPP-A levels and embryonic measurements of crown-rum\n",
      "Clean:\n",
      "objective prior foetal karyotyping likelihood downs syndrome often determined combine maternal age serum free betahcg pappa level embryonic measurement crownrump length nuchal translucency gestational\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "In this study, we present our findings in creating new first trimester crown-rump length equations that have been optimized through structured data collection from a French general population. Early p\n",
      "Clean:\n",
      "study present finding create new first trimester crownrump length equation optimize structure data collection french general population early prenatal care essential ensure wellbeing mother fetus one \n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "We have combined the results from our recent observations of Damped and sub-Damped Lyman-alpha systems with the MIKE and UVES spectrographs on the Magellan Clay and VLT Kueyen telescopes with ones fro\n",
      "Clean:\n",
      "combine result recent observation damped subdamped lymanalpha system mike uves spectrograph magellan clay vlt kueyen telescope one literature determine nhiweighted mean metallicity system base fe depl\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "Recent observations indicate that low-mass galaxies may play a crucial role in the star formation history of the universe. In this context, we present a survey of sub-damped Lyman-Alpha (sub-DLA) syst\n",
      "Clean:\n",
      "recent observation indicate lowmass galaxy may play crucial role star formation history universe context present survey subdamped lymanalpha subdla system redshifts z15 trace neutral gas content lowma\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "We prove that some subquotient categories of exact categories are abelian.\n",
      "\n",
      "This generalizes a result by Koenig-Zhu in the case of (algebraic) triangulated categories. As a particular case, if an exac\n",
      "Clean:\n",
      "prove subquotient category exact category abelian generalize result koenigzhu case algebraic triangulated category particular case exact category b enough projectives injectives cluster tilt subcatego\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "We study the quotient categories of exact categories by cluster tilting subcategories, which are naturally module categories. We investigate the relationship between these categories and their cluster\n",
      "Clean:\n",
      "study quotient category exact category cluster tilt subcategories naturally module category investigate relationship category clustertilting object show object act generator work lead complete underst\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Colloids and proteins alike can bind to lipid bilayers and move laterally in these two-dimensional fluids. Inspired by proteins that generate membrane curvature, sense the underlying membrane geometry\n",
      "Clean:\n",
      "colloid protein alike bind lipid bilayers move laterally twodimensional fluid inspire protein generate membrane curvature sense underlie membrane geometry migrate high curvature sit explore question c\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This research work investigates the curvature-driven migration of colloids on lipid bilayers. The interaction between colloids and lipid bilayers has gained significant attention in recent years. The \n",
      "Clean:\n",
      "research work investigate curvaturedriven migration colloid lipid bilayers interaction colloid lipid bilayers gain significant attention recent year curvature lipid bilayers act driving force colloida\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Uropathogenic Escherichia coli (UPEC) express various kinds of organelles, so-called pili or fimbriae, that mediate adhesion to host tissue in the urinary tract through specific receptor-adhesin inter\n",
      "Clean:\n",
      "uropathogenic escherichia coli upec express various kind organelle socalled pili fimbria mediate adhesion host tissue urinary tract specific receptoradhesin interaction biomechanical property pili con\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "F1C pili are filamentous surface appendages produced by bacteria that play a crucial role in biofilm formation and bacterial pathogenesis. Understanding the biomechanical properties of F1C pili is key\n",
      "Clean:\n",
      "f1c pili filamentous surface appendage produce bacteria play crucial role biofilm formation bacterial pathogenesis understand biomechanical property f1c pili key develop strategy prevent bacterial inf\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "It is well-known that localized topological defects (solitons) experience recoil when they suffer an impact by incident particles. Higher-dimensional topological defects develop distinctive wave patte\n",
      "Clean:\n",
      "wellknown localized topological defect soliton experience recoil suffer impact incident particle higherdimensional topological defect develop distinctive wave pattern propagate along worldvolume simil\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "We investigate the phenomenon of local recoil in extended solitons, focusing on a particular example from string theory. Solitons are stable solutions of nonlinear field equations that can propagate w\n",
      "Clean:\n",
      "investigate phenomenon local recoil extended soliton focus particular example string theory soliton stable solution nonlinear field equation propagate without change shape play important role many are\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Context: Database-backed applications often run queries with more authority than necessary. Since programs can access more data than they legitimately need, flaws in security checks at the application\n",
      "Clean:\n",
      "context databasebacked application often run query authority necessary since program access data legitimately need flaw security check application level enable malicious buggy code view modify data vi\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "Fine-grained, language-based access control for database-backed applications refers to a security mechanism that offers database access based on predefined language constructs. The aim of this researc\n",
      "Clean:\n",
      "finegrained languagebased access control databasebacked application refers security mechanism offer database access base predefined language construct aim research paper investigate effectiveness fine\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "In string musical instruments, the sound is radiated by the soundboard, subject to the strings excitation. This vibration of this rather complex structure is described here with models which need only\n",
      "Clean:\n",
      "string musical instrument sound radiate soundboard subject string excitation vibration rather complex structure describe model need small number parameter prediction model compare result experiment pr\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The soundboard of a piano plays a vital role in the production of its distinctive sound. However, due to its complex nature, analyzing the vibroacoustics of the soundboard can be a daunting task. This\n",
      "Clean:\n",
      "soundboard piano play vital role production distinctive sound however due complex nature analyze vibroacoustics soundboard daunt task research paper present reduce model piano soundboard aid analysis \n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "An energy-harvesting sensor node that is sending status updates to a destination is considered. The sensor is equipped with a battery of finite size to save its incoming energy, and consumes one unit \n",
      "Clean:\n",
      "energyharvesting sensor node send status update destination consider sensor equip battery finite size save incoming energy consume one unit energy per status update transmission deliver destination in\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This paper focuses on the problem of age-minimal transmission for energy harvesting sensors with finite batteries. In order to conserve energy and prolong the lifetime of the sensor, the focus is on d\n",
      "Clean:\n",
      "paper focus problem ageminimal transmission energy harvest sensor finite battery order conserve energy prolong lifetime sensor focus develop online policy make intelligent decision transmit informatio\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Multimedia scenarios have multimedia content and interactive events associated with computer programs. Interactive Scores (IS) is a formalism to represent such scenarios by temporal objects, temporal \n",
      "Clean:\n",
      "multimedia scenario multimedia content interactive event associate computer program interactive score formalism represent scenario temporal object temporal relation trs interactive event describe trs \n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The paper proposes a new formalism called Concurrent Constraint Conditional-Branching Timed Interactive Scores (C3BTIS) for specifying interactive multimedia works. C3BTIS models the timing and intera\n",
      "Clean:\n",
      "paper propose new formalism call concurrent constraint conditionalbranching time interactive score c3btis specify interactive multimedia work c3btis model timing interaction pattern multimedia work wa\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "We present the first all-sky view of the Sagittarius (Sgr) dwarf galaxy mapped by M giant star tracers detected in the complete Two Micron All-Sky Survey (2MASS). The main body is fit with a King prof\n",
      "Clean:\n",
      "present first allsky view sagittarius sgr dwarf galaxy map giant star tracer detect complete two micron allsky survey 2mass main body fit king profile 30 deg limit radius break density profile star ti\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The Sagittarius Dwarf Galaxy is located in the important dwarf galaxy population of the Milky Way halo. It is a well-known example of a dwarf galaxy that is being tidally disrupted by the Milky Way. T\n",
      "Clean:\n",
      "sagittarius dwarf galaxy locate important dwarf galaxy population milky way halo wellknown example dwarf galaxy tidally disrupt milky way improve understanding morphology sagittarius core tidal arm co\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "An alternative treatment is proposed for the calculations carried out within the frame of Nikiforov-Uvarov method, which removes a drawback in the original theory and by pass some difficulties in solv\n",
      "Clean:\n",
      "alternative treatment propose calculation carry within frame nikiforovuvarov method remove drawback original theory pas difficulty solve schrodinger equation present procedure illustrate example ortho\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The Nikiforov-Uvarov (NU) method has been extensively studied for solving Schrödinger's equation under various conditions. This search presents a comprehensive review of the NU formalism and its appli\n",
      "Clean:\n",
      "nikiforovuvarov nu method extensively study solve schrödingers equation various condition search present comprehensive review nu formalism application mathematical physic analysis provide deep underst\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "A variety of representation learning approaches have been investigated for reinforcement learning; much less attention, however, has been given to investigating the utility of sparse coding. Outside o\n",
      "Clean:\n",
      "variety representation learn approach investigate reinforcement learn much less attention however give investigate utility sparse cod outside reinforcement learn sparse cod representation widely use n\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "Sparse coding has been successfully used in various machine learning applications including image and audio processing. In this paper, we introduce a novel approach for learning sparse representations\n",
      "Clean:\n",
      "sparse coding successfully use various machine learning application include image audio processing paper introduce novel approach learn sparse representation reinforcement learn propose method combine\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "We establish one-dimensional spectral stability of small amplitude viscous and relaxation shock profiles using Evans function techniques to perform a series of reductions and normal forms to reduce to\n",
      "Clean:\n",
      "establish onedimensional spectral stability small amplitude viscous relaxation shock profile use evans function technique perform series reduction normal form reduce case scalar burger equation multid\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "We present a novel Evans function method to analyze the spectral stability of small-amplitude shock profiles. By considering the linear stability problem, we show that the sign of the Evans function d\n",
      "Clean:\n",
      "present novel evans function method analyze spectral stability smallamplitude shock profile consider linear stability problem show sign evans function determine stability shock profile approach offer \n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "We investigate the variation of the gas and the radiation pressure in accretion disks during the infall of matter to the black hole and its effect to the flow. While the flow far away from the black h\n",
      "Clean:\n",
      "investigate variation gas radiation pressure accretion disk infall matter black hole effect flow flow far away black hole might nonrelativistic vicinity black hole expect relativistic behave like radi\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The study of the sub-Keplerian accretion disk around black holes has been a topic of great interest in astrophysics due to its implications for understanding the dynamics of black holes. This research\n",
      "Clean:\n",
      "study subkeplerian accretion disk around black hole topic great interest astrophysics due implication understand dynamic black hole research aim analyze variation gas radiation content within disk det\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "A new, unbiased Spitzer-MIPS imaging survey (~1.8 square degs) of the young stellar content of the Vela Molecular Cloud-D is presented. The survey is complete down to 5mJy and 250mJy at 24micron (mu) \n",
      "Clean:\n",
      "new unbiased spitzermips image survey 18 square degs young stellar content vela molecular cloudd present survey complete 5mjy 250mjy 24micron mu 70mu respectively 849 source detect 24mu 52 also 70mu c\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The Vela Molecular Cloud-D has long been of interest to astronomers, owing to its young stellar content and potential for observing the early stages of star formation. In this paper, we report on a Sp\n",
      "Clean:\n",
      "vela molecular cloudd long interest astronomer owe young stellar content potential observe early stage star formation paper report spitzer space telescope survey vela molecular cloudd use multiband im\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "The nearby star Fomalhaut harbours a cold, moderately eccentric dust belt with a sharp inner edge near 133 au. A low-mass, common proper motion companion (Fom b), was discovered near the inner edge an\n",
      "Clean:\n",
      "nearby star fomalhaut harbour cold moderately eccentric dust belt sharp inner edge near 133 au lowmass common proper motion companion fom b discover near inner edge identify planet candidate could acc\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This study presents an independent determination of Fomalhaut b's orbit and investigates its dynamical effects on the outer dust belt. Fomalhaut b is a directly imaged exoplanet located in the Fomalha\n",
      "Clean:\n",
      "study present independent determination fomalhaut bs orbit investigate dynamical effect outer dust belt fomalhaut b directly image exoplanet locate fomalhaut system approximately 25 lightyears away ea\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "We investigate novel parameter estimation and goodness-of-fit (GOF) assessment methods for large-scale confirmatory item factor analysis (IFA) with many respondents, items, and latent factors. For par\n",
      "Clean:\n",
      "investigate novel parameter estimation goodnessoffit gof assessment method largescale confirmatory item factor analysis ifa many respondent item latent factor parameter estimation extend urban bauers \n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This paper introduces a novel approach to confirmatory item factor analysis (CFA) that relies on machine learning-based estimation and goodness-of-fit techniques. The proposed methodology addresses th\n",
      "Clean:\n",
      "paper introduce novel approach confirmatory item factor analysis cfa rely machine learningbased estimation goodnessoffit technique propose methodology address demand largescale cfa traditional method \n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "This work suggests an interpolation-based stochastic collocation method for the non-intrusive and adaptive construction of sparse polynomial chaos expansions (PCEs). Unlike pseudo-spectral projection \n",
      "Clean:\n",
      "work suggest interpolationbased stochastic collocation method nonintrusive adaptive construction sparse polynomial chaos expansion pces unlike pseudospectral projection regressionbased stochastic coll\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The current work presents a novel approach towards constructing adaptive sparse polynomial chaos expansions through the application of Leja interpolation. Using advanced mathematical techniques, we ha\n",
      "Clean:\n",
      "current work present novel approach towards construct adaptive sparse polynomial chaos expansion application leja interpolation use advanced mathematical technique develop framework construct polynomi\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "We analyze the possibility that several instability points may be formed, due to the Paczy\\'nski mechanism of violation of mechanical equilibrium, in the orbiting matter around a supermassive Kerr bla\n",
      "Clean:\n",
      "analyze possibility several instability point may form due paczynski mechanism violation mechanical equilibrium orbiting matter around supermassive kerr black hole consider recently propose model ring\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The formation and dynamics of ringed accretion disks around astrophysical objects have been the subject of extensive research. Unstable behavior within these systems has been observed and analyzed. No\n",
      "Clean:\n",
      "formation dynamic ringed accretion disk around astrophysical object subject extensive research unstable behavior within system observe analyze nonaxisymmetric perturbation whether external intrinsic d\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "We propose a spurious region guided refinement approach for robustness verification of deep neural networks. Our method starts with applying the DeepPoly abstract domain to analyze the network. If the\n",
      "Clean:\n",
      "propose spurious region guide refinement approach robustness verification deep neural network method start apply deeppoly abstract domain analyze network robustness property verify result inconclusive\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "Deep neural networks are widely used in applications including image recognition, natural language processing, and autonomous driving. However, ensuring the reliability and robustness of these network\n",
      "Clean:\n",
      "deep neural network widely use application include image recognition natural language processing autonomous drive however ensure reliability robustness network remain challenge address challenge propo\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "It is well-known that for infinitely repeated games, there are computable strategies that have best responses, but no computable best responses. These results were originally proved for either specifi\n",
      "Clean:\n",
      "wellknown infinitely repeat game computable strategy best response computable best response result originally prove either specific game eg prisoner dilemma class game satisfy certain condition know n\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "In this paper, we provide a thorough analysis of infinitely repeated two-player games, where the strategies are computable but have no computable best response under a Limit-of-Means payoff. We establ\n",
      "Clean:\n",
      "paper provide thorough analysis infinitely repeat twoplayer game strategy computable computable best response limitofmeans payoff establish condition nonexistence computable best response provide comp\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Graphs are commonly used to characterise interactions between objects of interest. Because they are based on a straightforward formalism, they are used in many scientific fields from computer science \n",
      "Clean:\n",
      "graph commonly use characterise interaction object interest base straightforward formalism use many scientific field computer science historical science paper give introduction method rely graph learn\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "Graphs have emerged as a powerful tool in the field of machine learning due to their ability to capture and represent complex data relationships. In this paper, we introduce the fundamentals of graphs\n",
      "Clean:\n",
      "graph emerge powerful tool field machine learn due ability capture represent complex data relationship paper introduce fundamental graph application machine learning begin discuss basic graph theory i\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "An open question in quantum optics is how to manipulate and control complex quantum states in an experimentally feasible way. Here we present concepts for transformations of high-dimensional multi-pho\n",
      "Clean:\n",
      "open question quantum optic manipulate control complex quantum state experimentally feasible way present concept transformation highdimensional multiphotonic quantum system proposal rely two new idea \n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "In this paper, we propose a novel computer-inspired concept for implementing high-dimensional multipartite quantum gates. Our approach takes advantage of recent advances in quantum computing to design\n",
      "Clean:\n",
      "paper propose novel computerinspired concept implement highdimensional multipartite quantum gate approach take advantage recent advance quantum compute design faulttolerant gate manipulate multiple qu\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Understanding visual reality involves acquiring common-sense knowledge about countless regularities in the visual world, e.g., how illumination alters the appearance of objects in a scene, and how mot\n",
      "Clean:\n",
      "understand visual reality involve acquire commonsense knowledge countless regularity visual world eg illumination alter appearance object scene motion change apparent spatial relationship regularity h\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This paper presents an approach to unsupervised learning in an environment of continuous video streams using a scalable and predictive recurrent neural network. The aim of this study is to develop a m\n",
      "Clean:\n",
      "paper present approach unsupervised learning environment continuous video stream use scalable predictive recurrent neural network aim study develop model capable discover learn underlying structure vi\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Recent advances in gravitational-wave astronomy make the direct detection of gravitational waves from the merger of two stellar-mass compact objects a realistic prospect. Evolutionary scenarios toward\n",
      "Clean:\n",
      "recent advance gravitationalwave astronomy make direct detection gravitational wave merger two stellarmass compact object realistic prospect evolutionary scenario towards merger double compact object \n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The merger of massive black holes is a striking astrophysical phenomenon that continuously puzzles astronomers, particularly as the gravitational waves produced by these events can be detected by adva\n",
      "Clean:\n",
      "merger massive black hole striking astrophysical phenomenon continuously puzzle astronomer particularly gravitational wave produce event detect advanced instrument ligo virgo work explore new route to\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Now a day computer is necessary for human being and it is very useful in many fields like search engine, text processing, short messaging services, voice chatting and text recognition. Since last many\n",
      "Clean:\n",
      "day computer necessary human useful many field like search engine text process short messaging service voice chat text recognition since last many year many tool technique develop support writing lang\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The aim of this paper is to propose a contextual approach to Roman-Urdu to Urdu script transliteration. The need for this system arises due to the widespread usage of Roman-Urdu in digital communicati\n",
      "Clean:\n",
      "aim paper propose contextual approach romanurdu urdu script transliteration need system arise due widespread usage romanurdu digital communication whereas urdu script remain preferred choice official \n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "This study analyzes the factors affecting the configuration and consolidation of green ports in Colombia. For this purpose a case stady of maritime cargo ports of Cartagena, Barranquilla and Santa Mar\n",
      "Clean:\n",
      "study analyze factor affect configuration consolidation green port colombia purpose case stady maritime cargo port cartagena barranquilla santa marta perform address semiestructured interview identify\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This research paper aims to analyze the primary factors influencing the creation of green ports in Colombia, addressing the challenges and opportunities generated by environmental concerns, port stake\n",
      "Clean:\n",
      "research paper aim analyze primary factor influence creation green port colombia address challenge opportunity generate environmental concern port stakeholder expectation global market demand achieve \n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "We present a quantitative analysis of Yang-Mills thermodynamics in 4D flat spacetime. The focus is on the gauge group SU(2). Results for SU(3) are mentioned in passing. Although all essential argument\n",
      "Clean:\n",
      "present quantitative analysis yangmills thermodynamics 4d flat spacetime focus gauge group su2 result su3 mention pass although essential argument result report elsewhere summarize concise way offer n\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This paper reviews the thermodynamics of the Yang-Mills theory. The approach is based on the recent developments in quantum chromodynamics using lattice simulations. We address the underlying assumpti\n",
      "Clean:\n",
      "paper review thermodynamics yangmills theory approach base recent development quantum chromodynamics use lattice simulation address underlying assumption physical interpretation thermodynamic quantity\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Airline Crew Pairing Optimization (CPO) aims at generating a set of legal flight sequences (crew pairings), to cover an airline's flight schedule, at minimum cost. It is usually performed using Column\n",
      "Clean:\n",
      "airline crew pair optimization cpo aim generate set legal flight sequence crew pairing cover airline flight schedule minimum cost usually perform use column generation cg mathematical programming tech\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "The airline industry is a complex system with many challenges to optimize operations. One of the main challenges is the crew pairing optimization, which is the process of assigning a crew to a sequenc\n",
      "Clean:\n",
      "airline industry complex system many challenge optimize operation one main challenge crew pair optimization process assign crew sequence flight problem become particularly difficult deal largescale ai\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "A large class of unsupervised algorithms for Word Sense Disambiguation (WSD) is that of dictionary-based methods. Various algorithms have as the root Lesk's algorithm, which exploits the sense definit\n",
      "Clean:\n",
      "large class unsupervised algorithm word sense disambiguation wsd dictionarybased method various algorithm root lesks algorithm exploit sense definition dictionary directly approach use lexical base wo\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "This paper presents a novel technique for Word Sense Disambiguation (WSD), called the chain dictionary method. The approach involves the use of a chain of dictionaries to improve WSD performance by in\n",
      "Clean:\n",
      "paper present novel technique word sense disambiguation wsd call chain dictionary method approach involve use chain dictionary improve wsd performance incorporate information multiple source method ev\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "Diffuse intracluster light (ICL) and cD galaxy halos are believed to originate from galaxy evolution and disruption. We present a kinematic study of the ICL in the Hydra I cluster core, using planetar\n",
      "Clean:\n",
      "diffuse intracluster light icl cd galaxy halo believe originate galaxy evolution disruption present kinematic study icl hydra cluster core use planetary nebula pns tracer use multislit image spectrosc\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "In this study, we investigate the kinematics and origins of the diffuse stellar light found in the core of the Hydra I cluster (Abell 1060). Through deep imaging, we were able to analyze the propertie\n",
      "Clean:\n",
      "study investigate kinematics origin diffuse stellar light find core hydra cluster abell 1060 deep imaging able analyze property diffuse light distinguish light produce individual star find diffuse lig\n",
      "________\n",
      "\n",
      "Raw (Human):\n",
      "We report the results of a preliminary nova survey of Local Group dwarf ellipticals. We used the Tenagra Observatory to observe M32, NGC 205, NGC 147, and NGC 185 in their entirety every clear night o\n",
      "Clean:\n",
      "report result preliminary nova survey local group dwarf ellipticals use tenagra observatory observe m32 ngc 205 ngc 147 ngc 185 entirety every clear night 45 month interval discover one nova m32 candi\n",
      "________\n",
      "\n",
      "Raw (AI):\n",
      "Recent studies have suggested that the nearby Local Group dwarf galaxies M32 and NGC 205 may experience a high rate of nova occurrences. To investigate this possibility, we have conducted a detailed a\n",
      "Clean:\n",
      "recent study suggest nearby local group dwarf galaxy m32 ngc 205 may experience high rate nova occurrence investigate possibility conduct detailed analysis archival data hubble space telescope groundb\n",
      "________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# comparing raw and clean texts\n",
    "for idx, row in filtered.head(100).iterrows():\n",
    "\n",
    "    if row.label == 1:\n",
    "        #print('Raw (AI):')\n",
    "    \n",
    "    else:\n",
    "        #print('Raw (Human):')\n",
    "      \n",
    "    #print(row.text[0:200])\n",
    "    #print('Clean:')   \n",
    "    #print(row.clean_text[0:200])\n",
    "    #print('________') \n",
    "    #print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abelian regular coverings of the quaternion hypermap have been a topic of interest in graph theory and combinatorics for years. In this paper, we aim to provide a thorough analysis of the properties and behavior of these coverings. We introduce a new method to construct these coverings using projections onto a suitable plane. Through our analysis, we discover that the hypermaps subject to these coverings exhibit symmetries that are directly related to the covering group. Furthermore, we investigate the effect of various transformations on the quaternion hypermap, such as automorphisms and isomorphisms. Our findings reveal surprising relationships between different coverings and the underlying hypermap structure. Additionally, we discuss the connections between abelian regular coverings and other areas of mathematics, such as algebraic topology and group theory. The results presented in this paper contribute to a deeper understanding of the relationship between coverings and hypermaps and their properties.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered[filtered.apply(lambda row: row['text'].startswith(row['title']), axis=1)]['text'].iloc[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Vectorizer: tfidf | N-gram: (1, 1) ===\n",
      "\n",
      "Model: logreg\n",
      "| CV Acc: 0.946 ± 0.004 | Test Acc: 0.943 | Features: 3885\n",
      "\n",
      "Model: nbayes\n",
      "| CV Acc: 0.901 ± 0.009 | Test Acc: 0.889 | Features: 3885\n",
      "\n",
      "Model: xgboost\n",
      "| CV Acc: 0.948 ± 0.004 | Test Acc: 0.946 | Features: 3885\n",
      "\n",
      "=== Vectorizer: tfidf | N-gram: (1, 2) ===\n",
      "\n",
      "Model: logreg\n",
      "| CV Acc: 0.953 ± 0.004 | Test Acc: 0.955 | Features: 6860\n",
      "\n",
      "Model: nbayes\n",
      "| CV Acc: 0.939 ± 0.005 | Test Acc: 0.942 | Features: 6860\n",
      "\n",
      "Model: xgboost\n",
      "| CV Acc: 0.949 ± 0.003 | Test Acc: 0.951 | Features: 6860\n",
      "\n",
      "=== Vectorizer: tfidf | N-gram: (2, 2) ===\n",
      "\n",
      "Model: logreg\n",
      "| CV Acc: 0.929 ± 0.006 | Test Acc: 0.923 | Features: 2975\n",
      "\n",
      "Model: nbayes\n",
      "| CV Acc: 0.887 ± 0.006 | Test Acc: 0.886 | Features: 2975\n",
      "\n",
      "Model: xgboost\n",
      "| CV Acc: 0.913 ± 0.004 | Test Acc: 0.917 | Features: 2975\n",
      "\n",
      "=== Vectorizer: count | N-gram: (1, 1) ===\n",
      "\n",
      "Model: logreg\n",
      "| CV Acc: 0.942 ± 0.003 | Test Acc: 0.948 | Features: 3885\n",
      "\n",
      "Model: nbayes\n",
      "| CV Acc: 0.919 ± 0.006 | Test Acc: 0.906 | Features: 3885\n",
      "\n",
      "Model: xgboost\n",
      "| CV Acc: 0.951 ± 0.004 | Test Acc: 0.950 | Features: 3885\n",
      "\n",
      "=== Vectorizer: count | N-gram: (1, 2) ===\n",
      "\n",
      "Model: logreg\n",
      "| CV Acc: 0.954 ± 0.003 | Test Acc: 0.958 | Features: 6860\n",
      "\n",
      "Model: nbayes\n",
      "| CV Acc: 0.946 ± 0.004 | Test Acc: 0.941 | Features: 6860\n",
      "\n",
      "Model: xgboost\n",
      "| CV Acc: 0.952 ± 0.003 | Test Acc: 0.955 | Features: 6860\n",
      "\n",
      "=== Vectorizer: count | N-gram: (2, 2) ===\n",
      "\n",
      "Model: logreg\n",
      "| CV Acc: 0.934 ± 0.006 | Test Acc: 0.922 | Features: 2975\n",
      "\n",
      "Model: nbayes\n",
      "| CV Acc: 0.889 ± 0.008 | Test Acc: 0.888 | Features: 2975\n",
      "\n",
      "Model: xgboost\n",
      "| CV Acc: 0.917 ± 0.007 | Test Acc: 0.916 | Features: 2975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# vectorization configs\n",
    "vectorizer_configs = [\n",
    "    ('tfidf', (1, 1)),\n",
    "    ('tfidf', (1, 2)),\n",
    "    ('tfidf', (2, 2)),\n",
    "    ('count', (1, 1)),\n",
    "    ('count', (1, 2)),\n",
    "    ('count', (2, 2)),\n",
    "]\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    'logreg': LogisticRegression(max_iter=1000),\n",
    "    'nbayes': MultinomialNB(),\n",
    "    'xgboost': XGBClassifier(eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Train-test split\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
    "    filtered['clean_text'], filtered['label'], test_size=0.2, random_state=15, stratify=filtered['label']\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n",
    "\n",
    "# Storage dictionary\n",
    "trained_models = {}\n",
    "\n",
    "\n",
    "for vec_type, ngram_range in vectorizer_configs:\n",
    "    print(f'\\n=== Vectorizer: {vec_type} | N-gram: {ngram_range} ===')\n",
    "\n",
    "    VectorizerClass = TfidfVectorizer if vec_type == 'tfidf' else CountVectorizer\n",
    "    \n",
    "\n",
    "    # Models\n",
    "    for model_name, model in models.items():\n",
    "        print(f'\\nModel: {model_name}')\n",
    "\n",
    "\n",
    "        # vectorization\n",
    "        vectorizer = VectorizerClass(ngram_range=ngram_range, max_df=0.8, min_df=20)\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "        # CV\n",
    "        cv_scores = cross_val_score(\n",
    "            pipeline,\n",
    "            X_train_texts,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        mean_cv = np.mean(cv_scores)\n",
    "        std_cv = np.std(cv_scores)\n",
    "\n",
    "        # Refitting on training split\n",
    "        pipeline.fit(X_train_texts, y_train)\n",
    "        y_pred = pipeline.predict(X_test_texts)\n",
    "\n",
    "        test_acc = accuracy_score(y_test, y_pred)\n",
    "        clf_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        vocab_size = len(pipeline.named_steps['vectorizer'].vocabulary_)\n",
    "\n",
    "        print(f\"| CV Acc: {mean_cv:.3f} ± {std_cv:.3f} | Test Acc: {test_acc:.3f} | Features: {vocab_size}\")\n",
    "\n",
    "        # modellek mentése mert később kellenek egyéb predikciókhoz\n",
    "        key = f\"{model_name}_{vec_type}_{ngram_range}\"\n",
    "        trained_models[key] = {\n",
    "            'pipeline': copy.deepcopy(pipeline),\n",
    "            'vectorizer_type': vec_type,\n",
    "            'ngram_range': ngram_range,\n",
    "            'cv_mean_accuracy': mean_cv,\n",
    "            'cv_std': std_cv,\n",
    "            'test_accuracy': test_acc,\n",
    "            'classification_report': clf_report,\n",
    "            'vocab_size': vocab_size,\n",
    "            'y_pred': y_pred,                       \n",
    "            'y_true': y_test.values                 \n",
    "        }\n",
    "        \n",
    "\n",
    "        del pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg\n",
      "accuracy: 0.958203125\n",
      "{'precision': 0.9643705463182898, 'recall': 0.9515625, 'f1-score': 0.9579237121510028, 'support': 1280.0}\n",
      "\n",
      "nbayes\n",
      "accuracy: 0.94140625\n",
      "{'precision': 0.9366306027820711, 'recall': 0.946875, 'f1-score': 0.9417249417249417, 'support': 1280.0}\n",
      "\n",
      "xgboost\n",
      "accuracy: 0.95546875\n",
      "{'precision': 0.9619651347068146, 'recall': 0.9484375, 'f1-score': 0.955153422501967, 'support': 1280.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in ['logreg', 'nbayes', 'xgboost']:\n",
    "\n",
    "    print(key)\n",
    "    print(f'accuracy: {trained_models[f'{key}_count_(1, 2)']['classification_report']['accuracy']}')\n",
    "    print(f'{trained_models[f'{key}_count_(1, 2)']['classification_report']['1']}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Correct by all three: 2322\n",
      " Misclassified by all three: 42\n",
      " Disagreement among models: 196\n"
     ]
    }
   ],
   "source": [
    "# Filter only the models with CountVectorizer and (1,2) n-gram\n",
    "target_models = ['logreg_count_(1, 2)', 'nbayes_count_(1, 2)', 'xgboost_count_(1, 2)']\n",
    "\n",
    "# Extract predictions and true labels\n",
    "preds = [trained_models[model_key]['y_pred'] for model_key in target_models]\n",
    "true_labels = trained_models[target_models[0]]['y_true']  # same for all\n",
    "\n",
    "# Convert to numpy array for vectorized comparison\n",
    "preds_array = np.array(preds)  # shape: (3, num_samples)\n",
    "true_array = np.array(true_labels)  # shape: (num_samples,)\n",
    "\n",
    "# Transpose preds_array so that shape becomes (num_samples, 3)\n",
    "preds_array = preds_array.T\n",
    "\n",
    "# Now compare\n",
    "correct_by_all = np.all(preds_array == true_array[:, None], axis=1)\n",
    "wrong_by_all = np.all(preds_array != true_array[:, None], axis=1)\n",
    "disagreement = ~(correct_by_all | wrong_by_all)\n",
    "\n",
    "# Count each category\n",
    "num_correct_all = np.sum(correct_by_all)\n",
    "num_wrong_all = np.sum(wrong_by_all)\n",
    "num_disagreement = np.sum(disagreement)\n",
    "\n",
    "print(f\" Correct by all three: {num_correct_all}\")\n",
    "print(f\" Misclassified by all three: {num_wrong_all}\")\n",
    "print(f\" Disagreement among models: {num_disagreement}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with texts generated by other LLM-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data = pd.read_csv(\"other_llm_test_data/llm_final_data_2.csv\", sep=\";\", encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data['clean_text'] = llm_data['text'].astype(str).apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      propargyl radical c3h3 represent critical reac...\n",
       "1      study address methodological challenge estimat...\n",
       "2      study investigate intermittency phenomenon two...\n",
       "3      comprehensive study investigate complex phenom...\n",
       "4      research present comprehensive computational m...\n",
       "                             ...                        \n",
       "115    study present exhaustive analysis charge occup...\n",
       "116    report multiple crossing landau level double h...\n",
       "117    study investigate role nonorthogonal wavefunct...\n",
       "118    proliferation networked technology give rise u...\n",
       "119    present first deep observation local interstel...\n",
       "Name: clean_text, Length: 120, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_data.clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model_keys = ['logreg_count_(1, 2)', 'nbayes_count_(1, 2)', 'xgboost_count_(1, 2)']\n",
    "\n",
    "for key in model_keys:\n",
    "\n",
    "\n",
    "    pipeline_llm = trained_models[key]['pipeline']\n",
    "\n",
    "\n",
    "    llm_data['clean_text'] = llm_data['clean_text'].fillna(\"\").astype(str)\n",
    "    preds = pipeline_llm.predict(llm_data['clean_text'])\n",
    "    probs = pipeline_llm.predict_proba(llm_data['clean_text'])\n",
    "\n",
    "    llm_data[f'predicted_label_{key}'] = preds\n",
    "    llm_data[f'prediction_prob_{key}'] = probs.max(axis=1) \n",
    "\n",
    "\n",
    "    #print(key)\n",
    "    #print(classification_report(llm_data['label'], preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted_label_logreg_count_(1, 2)</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Claude 3.5 Haiku</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-V3</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini 2.0 Flash</th>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama3.2</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted_label_logreg_count_(1, 2)         0         1\n",
       "author                                                 \n",
       "Claude 3.5 Haiku                          NaN  1.000000\n",
       "DeepSeek-V3                          0.333333  0.666667\n",
       "Gemini 2.0 Flash                     0.033333  0.966667\n",
       "llama3.2                             0.066667  0.933333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_data.groupby('author')['predicted_label_logreg_count_(1, 2)'].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted_label_nbayes_count_(1, 2)</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Claude 3.5 Haiku</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-V3</th>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini 2.0 Flash</th>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama3.2</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted_label_nbayes_count_(1, 2)         0         1\n",
       "author                                                 \n",
       "Claude 3.5 Haiku                          NaN  1.000000\n",
       "DeepSeek-V3                          0.633333  0.366667\n",
       "Gemini 2.0 Flash                     0.033333  0.966667\n",
       "llama3.2                             0.066667  0.933333"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_data.groupby('author')['predicted_label_nbayes_count_(1, 2)'].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted_label_xgboost_count_(1, 2)</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Claude 3.5 Haiku</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-V3</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini 2.0 Flash</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama3.2</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted_label_xgboost_count_(1, 2)         0         1\n",
       "author                                                  \n",
       "Claude 3.5 Haiku                           NaN  1.000000\n",
       "DeepSeek-V3                           0.300000  0.700000\n",
       "Gemini 2.0 Flash                      0.166667  0.833333\n",
       "llama3.2                              0.100000  0.900000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_data.groupby('author')['predicted_label_xgboost_count_(1, 2)'].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "llms = llm_data.author.unique().tolist()\n",
    "top_ngrams = pd.DataFrame()\n",
    "\n",
    "for llm in llms:\n",
    "    # Get the correct vectorizer from your trained pipeline\n",
    "    vectorizer = trained_models[key]['pipeline'].named_steps['vectorizer']\n",
    "\n",
    "    # Transform your data into document-term matrix\n",
    "    X_vec = vectorizer.transform(llm_data[llm_data.author == llm]['clean_text'])\n",
    "\n",
    "    # Sum frequencies across all documents\n",
    "    token_counts = np.array(X_vec.sum(axis=0)).flatten()\n",
    "\n",
    "    # Map feature indices to actual n-gram strings\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    ngram_freq = list(zip(vocab, token_counts))\n",
    "\n",
    "    # Sort by frequency\n",
    "    ngram_freq_sorted = sorted(ngram_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_ngrams[llm] = ngram_freq_sorted[:30]\n",
    "    # Display top N\n",
    "    top_n = 20\n",
    "    #for ngram, count in ngram_freq_sorted[:top_n]:\n",
    "        #print(f\"{ngram}: {int(count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claude 3.5 Haiku</th>\n",
       "      <th>DeepSeek-V3</th>\n",
       "      <th>Gemini 2.0 Flash</th>\n",
       "      <th>llama3.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(quantum, 40)</td>\n",
       "      <td>(system, 28)</td>\n",
       "      <td>(galaxy, 30)</td>\n",
       "      <td>(galaxy, 36)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(complex, 33)</td>\n",
       "      <td>(provide, 26)</td>\n",
       "      <td>(provide, 24)</td>\n",
       "      <td>(demonstrate, 27)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(technique, 33)</td>\n",
       "      <td>(demonstrate, 25)</td>\n",
       "      <td>(gas, 23)</td>\n",
       "      <td>(study, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(mechanism, 32)</td>\n",
       "      <td>(model, 25)</td>\n",
       "      <td>(investigate, 20)</td>\n",
       "      <td>(system, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(provide, 31)</td>\n",
       "      <td>(result, 24)</td>\n",
       "      <td>(property, 20)</td>\n",
       "      <td>(result, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(analysis, 30)</td>\n",
       "      <td>(show, 24)</td>\n",
       "      <td>(potential, 19)</td>\n",
       "      <td>(data, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(system, 30)</td>\n",
       "      <td>(analysis, 19)</td>\n",
       "      <td>(study, 19)</td>\n",
       "      <td>(structure, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(demonstrate, 29)</td>\n",
       "      <td>(galaxy, 19)</td>\n",
       "      <td>(analyze, 18)</td>\n",
       "      <td>(model, 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(dynamic, 29)</td>\n",
       "      <td>(framework, 18)</td>\n",
       "      <td>(distribution, 17)</td>\n",
       "      <td>(novel, 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(galaxy, 29)</td>\n",
       "      <td>(reveal, 18)</td>\n",
       "      <td>(molecular, 17)</td>\n",
       "      <td>(present, 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(research, 28)</td>\n",
       "      <td>(gas, 17)</td>\n",
       "      <td>(quasar, 17)</td>\n",
       "      <td>(complex, 19)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(molecular, 27)</td>\n",
       "      <td>(study, 17)</td>\n",
       "      <td>(molecular gas, 16)</td>\n",
       "      <td>(property, 19)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(critical, 26)</td>\n",
       "      <td>(present, 16)</td>\n",
       "      <td>(analysis, 15)</td>\n",
       "      <td>(star, 19)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(interaction, 26)</td>\n",
       "      <td>(data, 15)</td>\n",
       "      <td>(formation, 15)</td>\n",
       "      <td>(understanding, 19)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(formation, 24)</td>\n",
       "      <td>(use, 15)</td>\n",
       "      <td>(reveal, 15)</td>\n",
       "      <td>(provide, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(fundamental, 24)</td>\n",
       "      <td>(high, 14)</td>\n",
       "      <td>(demonstrate, 14)</td>\n",
       "      <td>(approach, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(insight, 24)</td>\n",
       "      <td>(method, 14)</td>\n",
       "      <td>(network, 14)</td>\n",
       "      <td>(evolution, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(approach, 23)</td>\n",
       "      <td>(approach, 13)</td>\n",
       "      <td>(approach, 13)</td>\n",
       "      <td>(formation, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(significant, 23)</td>\n",
       "      <td>(new, 13)</td>\n",
       "      <td>(data, 13)</td>\n",
       "      <td>(use, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(advanced, 21)</td>\n",
       "      <td>(distinct, 12)</td>\n",
       "      <td>(employ, 13)</td>\n",
       "      <td>(significant, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(framework, 21)</td>\n",
       "      <td>(flow, 12)</td>\n",
       "      <td>(evolution, 13)</td>\n",
       "      <td>(implication, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(across, 20)</td>\n",
       "      <td>(network, 12)</td>\n",
       "      <td>(explore, 13)</td>\n",
       "      <td>(insight, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(methodology, 20)</td>\n",
       "      <td>(transition, 12)</td>\n",
       "      <td>(present, 13)</td>\n",
       "      <td>(method, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(characteristic, 19)</td>\n",
       "      <td>(compare, 11)</td>\n",
       "      <td>(system, 13)</td>\n",
       "      <td>(network, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(gas, 19)</td>\n",
       "      <td>(complex, 11)</td>\n",
       "      <td>(utilize, 13)</td>\n",
       "      <td>(observation, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(process, 19)</td>\n",
       "      <td>(dynamic, 11)</td>\n",
       "      <td>(complex, 12)</td>\n",
       "      <td>(quasar, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(structure, 19)</td>\n",
       "      <td>(establish, 11)</td>\n",
       "      <td>(model, 12)</td>\n",
       "      <td>(research, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(employ, 18)</td>\n",
       "      <td>(group, 11)</td>\n",
       "      <td>(observed, 12)</td>\n",
       "      <td>(analysis, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(property, 18)</td>\n",
       "      <td>(quasar, 11)</td>\n",
       "      <td>(presence, 12)</td>\n",
       "      <td>(behavior, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(spectral, 18)</td>\n",
       "      <td>(state, 11)</td>\n",
       "      <td>(spatial, 12)</td>\n",
       "      <td>(dynamic, 13)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Claude 3.5 Haiku        DeepSeek-V3     Gemini 2.0 Flash  \\\n",
       "0          (quantum, 40)       (system, 28)         (galaxy, 30)   \n",
       "1          (complex, 33)      (provide, 26)        (provide, 24)   \n",
       "2        (technique, 33)  (demonstrate, 25)            (gas, 23)   \n",
       "3        (mechanism, 32)        (model, 25)    (investigate, 20)   \n",
       "4          (provide, 31)       (result, 24)       (property, 20)   \n",
       "5         (analysis, 30)         (show, 24)      (potential, 19)   \n",
       "6           (system, 30)     (analysis, 19)          (study, 19)   \n",
       "7      (demonstrate, 29)       (galaxy, 19)        (analyze, 18)   \n",
       "8          (dynamic, 29)    (framework, 18)   (distribution, 17)   \n",
       "9           (galaxy, 29)       (reveal, 18)      (molecular, 17)   \n",
       "10        (research, 28)          (gas, 17)         (quasar, 17)   \n",
       "11       (molecular, 27)        (study, 17)  (molecular gas, 16)   \n",
       "12        (critical, 26)      (present, 16)       (analysis, 15)   \n",
       "13     (interaction, 26)         (data, 15)      (formation, 15)   \n",
       "14       (formation, 24)          (use, 15)         (reveal, 15)   \n",
       "15     (fundamental, 24)         (high, 14)    (demonstrate, 14)   \n",
       "16         (insight, 24)       (method, 14)        (network, 14)   \n",
       "17        (approach, 23)     (approach, 13)       (approach, 13)   \n",
       "18     (significant, 23)          (new, 13)           (data, 13)   \n",
       "19        (advanced, 21)     (distinct, 12)         (employ, 13)   \n",
       "20       (framework, 21)         (flow, 12)      (evolution, 13)   \n",
       "21          (across, 20)      (network, 12)        (explore, 13)   \n",
       "22     (methodology, 20)   (transition, 12)        (present, 13)   \n",
       "23  (characteristic, 19)      (compare, 11)         (system, 13)   \n",
       "24             (gas, 19)      (complex, 11)        (utilize, 13)   \n",
       "25         (process, 19)      (dynamic, 11)        (complex, 12)   \n",
       "26       (structure, 19)    (establish, 11)          (model, 12)   \n",
       "27          (employ, 18)        (group, 11)       (observed, 12)   \n",
       "28        (property, 18)       (quasar, 11)       (presence, 12)   \n",
       "29        (spectral, 18)        (state, 11)        (spatial, 12)   \n",
       "\n",
       "               llama3.2  \n",
       "0          (galaxy, 36)  \n",
       "1     (demonstrate, 27)  \n",
       "2           (study, 25)  \n",
       "3          (system, 25)  \n",
       "4          (result, 24)  \n",
       "5            (data, 21)  \n",
       "6       (structure, 21)  \n",
       "7           (model, 20)  \n",
       "8           (novel, 20)  \n",
       "9         (present, 20)  \n",
       "10        (complex, 19)  \n",
       "11       (property, 19)  \n",
       "12           (star, 19)  \n",
       "13  (understanding, 19)  \n",
       "14        (provide, 18)  \n",
       "15       (approach, 17)  \n",
       "16      (evolution, 17)  \n",
       "17      (formation, 17)  \n",
       "18            (use, 17)  \n",
       "19    (significant, 16)  \n",
       "20    (implication, 14)  \n",
       "21        (insight, 14)  \n",
       "22         (method, 14)  \n",
       "23        (network, 14)  \n",
       "24    (observation, 14)  \n",
       "25         (quasar, 14)  \n",
       "26       (research, 14)  \n",
       "27       (analysis, 13)  \n",
       "28       (behavior, 13)  \n",
       "29        (dynamic, 13)  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ngrams.iloc[:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
